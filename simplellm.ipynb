{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Transformer from scratch in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning in /opt/homebrew/lib/python3.11/site-packages (2.5.0.post0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /opt/homebrew/lib/python3.11/site-packages (from lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]<2026.0,>=2022.5.0 in /opt/homebrew/lib/python3.11/site-packages (from lightning) (2023.12.2)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/homebrew/lib/python3.11/site-packages (from lightning) (0.11.9)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /Users/atefe/Library/Python/3.11/lib/python/site-packages (from lightning) (23.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /opt/homebrew/lib/python3.11/site-packages (from lightning) (2.1.2)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from lightning) (1.6.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/homebrew/lib/python3.11/site-packages (from lightning) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/homebrew/lib/python3.11/site-packages (from lightning) (4.9.0)\n",
      "Requirement already satisfied: pytorch-lightning in /opt/homebrew/lib/python3.11/site-packages (from lightning) (2.5.0.post0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.11)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (65.6.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch<4.0,>=2.1.0->lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch<4.0,>=2.1.0->lightning) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch<4.0,>=2.1.0->lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/homebrew/lib/python3.11/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Importing \"lightning\" to make it way easier to write our code  and for automatic code optimization and scaling in the cloud!\n",
    "! pip3 install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "from position_encoding import PositionEncoding\n",
    "from attention import Attention\n",
    "from decoder_only_transformer import DecoderOnlyTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "    What is Statquest? Awesome\n",
    "\n",
    "    Statquest is what? Awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {\n",
    "    \"what\": 0,\n",
    "    \"is\": 1,\n",
    "    \"statquest\": 2,\n",
    "    \"awesome\": 3,\n",
    "    \"<EOS>\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token = {id:token for token, id in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[token_to_id[\"what\"],\n",
    "                        token_to_id[\"is\"],\n",
    "                        token_to_id[\"statquest\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"]],\n",
    "\n",
    "                        [token_to_id[\"statquest\"],\n",
    "                         token_to_id[\"is\"],\n",
    "                         token_to_id[\"what\"],\n",
    "                         token_to_id[\"<EOS>\"],\n",
    "                         token_to_id[\"awesome\"]]])\n",
    "\n",
    "labels = torch.tensor([[token_to_id[\"is\"],\n",
    "                        token_to_id[\"statquest\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"],\n",
    "                        token_to_id[\"<EOS>\"]],\n",
    "\n",
    "                        [token_to_id[\"is\"],\n",
    "                        token_to_id[\"what\"],\n",
    "                        token_to_id[\"<EOS>\"],\n",
    "                        token_to_id[\"awesome\"],\n",
    "                        token_to_id[\"<EOS>\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionEncoding(nn.Module):\n",
    "#     def __init__(self, d_model=2, max_len=6):\n",
    "#         '''\n",
    "#         d_model: dimention of the model, number of word embedding values per token\n",
    "#         max_len: the maximum number of tokens our simpleLLm can process -- input and output combined\n",
    "        \n",
    "#         PE(pos,2i) = sin(pos/ 10000^(2i/model))\n",
    "#         PE(pos,2i+1) = cos(pos/ 10000^(2i/model))\n",
    "#         '''\n",
    "#         super().__init__()\n",
    "\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "#         position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "#         # torch.arange() = to create a sequence of numbers\n",
    "#         # unsqueeze(1) = turns the sequence to a column matrix\n",
    "#         embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "#         # setting step=2 results in the same sequence numbers that we would get if we multiplied i by 2. So we save ourselves a little math!\n",
    "        \n",
    "#         div_term = 1/torch.tensor(10000)**(embedding_index / d_model)\n",
    "\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "#         self.register_buffer('pe', pe)\n",
    "#         # To ensure that pe gets moved to a GPU if we use one!\n",
    "\n",
    "#     def forward(self, word_embeddings):\n",
    "#         return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, d_model=2):\n",
    "#         # d_model: number of word embedding values per token\n",
    "#         super().__init__()\n",
    "#         self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "#         self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "#         self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "#         # In the original Transformers manuscript, they don't add additional bias terms when calculating Attention, so we won't either by setting bias = False.\n",
    "#         # As a result, we end up with an object we're calling W_q with the currently untrained Weights needed to calculate Query values.\n",
    "       \n",
    "#         self.row_dim = 0\n",
    "#         self.col_dim = 1\n",
    "#         # Just to give us flexibility to input training data in sequentially or in batches, we creat some variables to keep track of which indices are for rows and columns.\n",
    "\n",
    "#     def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "\n",
    "#         ''' We are doing for the sake of flexibility is allowing the Query, Key and Values \n",
    "#             to be calculated from different token encodings.\n",
    "#         '''\n",
    "        \n",
    "#         q = self.W_q(encodings_for_q)\n",
    "#         k = self.W_q(encodings_for_k)\n",
    "#         v = self.W_q(encodings_for_v)\n",
    "\n",
    "#         sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "#         scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "#         if mask is not None:\n",
    "#             scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "#             # Masking is used to prevent early tokens from cheating and looking at later tokens.\n",
    "        \n",
    "#         attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "#         attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "#         return attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderOnlyTransformer(L.LightningModule):\n",
    "#     '''\n",
    "#     Doing it this way, rather than having every class inherit from\n",
    "#     LightningModule, allows us to take advantage of everything Lightning offers without the overhead of inheriting it multiple times.\n",
    "#     '''\n",
    "#     def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
    "#         # max_len: Maximum lenght of the input + output\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.we = nn.Embedding(num_embeddings= num_tokens, embedding_dim=d_model)\n",
    "#         self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "#         self.self_attention = Attention(d_model=d_model)\n",
    "#         self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "\n",
    "#         self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#     def forward(self, token_ids):\n",
    "#         word_embeddings = self.we(token_ids)\n",
    "#         position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "#         mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0))))\n",
    "#         # We create the mask that will prevent early tokens from looking at late tokens when we calculate Attention.\n",
    "#         mask=mask==0\n",
    "\n",
    "#         self_attention_values = self.self_attention(position_encoded,\n",
    "#                                                     position_encoded,\n",
    "#                                                     position_encoded,\n",
    "#                                                     mask=mask)\n",
    "        \n",
    "#         residual_connection_values = position_encoded + self_attention_values\n",
    "#         fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "\n",
    "#         return fc_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self):\n",
    "    return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "def training_step(self, batch, batch_idx):\n",
    "    input_tokens, labels = batch\n",
    "    output = self.forward(input_tokens[0])\n",
    "    loss = self.loss(output, labels[0]) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "No `training_step()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:932\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_callbacks()\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector\u001b[38;5;241m.\u001b[39m_attach_model_logging_functions()\n\u001b[0;32m--> 932\u001b[0m \u001b[43m_verify_loop_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;66;03m# SET UP THE TRAINER\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    937\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: setting up strategy environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:36\u001b[0m, in \u001b[0;36m_verify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected: Trainer state fn must be set before validating loop configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43m__verify_train_val_loop_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     __verify_manual_optimization_support(trainer, model)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mVALIDATING:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:53\u001b[0m, in \u001b[0;36m__verify_train_val_loop_configuration\u001b[0;34m(trainer, model)\u001b[0m\n\u001b[1;32m     51\u001b[0m has_training_step \u001b[38;5;241m=\u001b[39m is_overridden(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_training_step:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `training_step()` method defined. Lightning `Trainer` expects as minimum a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m has_optimizers \u001b[38;5;241m=\u001b[39m is_overridden(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigure_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_optimizers:\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: No `training_step()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined."
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "Predicted Tokens:\n",
      "\n",
      "\t what\n",
      "\t what\n",
      "\t is\n"
     ]
    }
   ],
   "source": [
    "model_input = torch.tensor([token_to_id[\"what\"],\n",
    "                            token_to_id[\"is\"],\n",
    "                            token_to_id[\"statquest\"],\n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "print(predicted_id)\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "max_length = 6\n",
    "for i in range(input_length, max_length) :\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "    \n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id) )\n",
    "\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print(\"\\t\", id_to_token[id.item()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
